{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd2042d",
   "metadata": {},
   "source": [
    "# ðŸ§  Deep Learning Fundamentals\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Types of Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Feedforward Neural Networks (FNN)**\n",
    "\n",
    "- **Structure**: Information flows in one direction (no loops or memory)\n",
    "- **Mathematical Expression**:\n",
    "  $$\n",
    "  y = f(W_n \\cdot f(W_{n-1} \\cdot \\dots f(W_1 \\cdot x + b_1) + \\dots + b_{n-1}) + b_n)\n",
    "  $$\n",
    "- **Activation Function**:\n",
    "  $$\n",
    "  a = \\sigma(Wx + b)\n",
    "  $$\n",
    "  where $\\sigma$ is typically ReLU, sigmoid, or tanh\n",
    "\n",
    "- **Use Cases**:\n",
    "  - Classification & regression on tabular data\n",
    "  - Feature extraction before other layers\n",
    "\n",
    "- **Limitation**:\n",
    "  - Cannot model temporal or spatial relationships in data\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Convolutional Neural Networks (CNN)**\n",
    "\n",
    "- **Core Idea**: Learn spatial hierarchies in images\n",
    "- **Key Operation â€“ Convolution**:\n",
    "  $$\n",
    "  \\text{FeatureMap}_{i,j} = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} \\text{Input}_{i+m,j+n} \\cdot \\text{Kernel}_{m,n}\n",
    "  $$\n",
    "\n",
    "- **Components**:\n",
    "  - **Convolution Layers**: Apply filters to detect features\n",
    "  - **Pooling Layers**: Downsample the feature maps (max/avg pooling)\n",
    "  - **Fully Connected Layers**: Final classification\n",
    "\n",
    "- **Use Cases**:\n",
    "  - Image classification (e.g., ImageNet)\n",
    "  - Object detection (YOLO, SSD)\n",
    "  - Medical imaging, facial recognition\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Recurrent Neural Networks (RNN)**\n",
    "\n",
    "- **Goal**: Model sequences and time-dependent data\n",
    "- **Hidden State Update**:\n",
    "  $$\n",
    "  h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\n",
    "  $$\n",
    "- **Output**:\n",
    "  $$\n",
    "  y_t = W_{hy}h_t + b_y\n",
    "  $$\n",
    "\n",
    "- **Problem**: Vanishing gradients with long sequences\n",
    "\n",
    "#### ðŸ” Variants:\n",
    "- **LSTM**: Uses memory cell and gates to retain long-term dependencies  \n",
    "  Forget Gate:\n",
    "  $$\n",
    "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "  $$\n",
    "\n",
    "- **GRU**: Simplified LSTM with fewer gates\n",
    "\n",
    "- **Use Cases**:\n",
    "  - Language modeling\n",
    "  - Time-series forecasting\n",
    "  - Speech recognition\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Transformer Networks**\n",
    "\n",
    "- **Self-Attention Mechanism**:\n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$\n",
    "\n",
    "- **Multi-Head Attention**:\n",
    "  $$\n",
    "  \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W^O\n",
    "  $$\n",
    "\n",
    "- **Positional Encoding**:\n",
    "  $$\n",
    "  PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "  $$\n",
    "\n",
    "- **Key Advantage**:\n",
    "  - Handles long-range dependencies\n",
    "  - Allows parallel training (unlike RNNs)\n",
    "\n",
    "- **Use Cases**:\n",
    "  - NLP (BERT, GPT)\n",
    "  - Vision Transformers (ViT)\n",
    "  - Multimodal models (CLIP, Flamingo)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Generative Adversarial Networks (GAN)**\n",
    "\n",
    "- **Two-Part System**:\n",
    "  - **Generator (G)**: Generates fake data\n",
    "  - **Discriminator (D)**: Distinguishes real from fake\n",
    "\n",
    "- **Minimax Objective**:\n",
    "  $$\n",
    "  \\min_G \\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]\n",
    "  $$\n",
    "\n",
    "- **Training Loop**:\n",
    "  1. Fix $G$, train $D$ to classify real vs fake\n",
    "  2. Fix $D$, train $G$ to fool $D$\n",
    "  3. Repeat until $D$ can't distinguish â†’ Nash equilibrium\n",
    "\n",
    "- **Use Cases**:\n",
    "  - Image generation (StyleGAN, BigGAN)\n",
    "  - Super-resolution\n",
    "  - Deepfakes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š History of Deep Learning\n",
    "\n",
    "### ðŸ”¹ Early Era (1940sâ€“1980s)\n",
    "- **1943**: McCulloch & Pitts model the first artificial neuron\n",
    "- **1958**: Rosenblatt proposes the perceptron\n",
    "- **1969**: Minsky & Papert show its limitations\n",
    "- **1982**: Hopfield networks\n",
    "\n",
    "### ðŸ”¹ Neural Net Revival (1986â€“1997)\n",
    "- **1986**: Backpropagation algorithm popularized\n",
    "- **1989**: LeNet-5 for digit recognition\n",
    "- **1997**: LSTM introduced (by Hochreiter & Schmidhuber)\n",
    "\n",
    "### ðŸ”¹ DL Breakthroughs (2006â€“Present)\n",
    "- **2006**: Hintonâ€™s Deep Belief Networks reignite deep learning\n",
    "- **2012**: AlexNet wins ImageNet â€” start of CNN revolution\n",
    "- **2014**: GANs introduced by Ian Goodfellow\n",
    "- **2017**: Transformers change NLP forever (\"Attention is All You Need\")\n",
    "- **2018**: BERT (Google) introduces bidirectional transformers\n",
    "- **2020**: GPT-3 (OpenAI) launches with 175B parameters\n",
    "- **2022â€“Present**: Foundation Models & ChatGPT redefine usability\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Applications of Deep Learning\n",
    "\n",
    "### ðŸ“· Computer Vision\n",
    "- Image classification: ResNet, EfficientNet\n",
    "- Object detection: YOLO, Faster R-CNN\n",
    "- Segmentation: U-Net, DeepLab\n",
    "- Medical imaging: CT, X-ray, MRI analysis\n",
    "\n",
    "### ðŸ’¬ Natural Language Processing\n",
    "- Text classification, sentiment analysis\n",
    "- Machine translation (e.g., Google Translate)\n",
    "- Question answering (e.g., BERT, RoBERTa)\n",
    "- Large Language Models (GPT, PaLM)\n",
    "\n",
    "### ðŸ”Š Audio & Speech\n",
    "- Speech-to-text (DeepSpeech, Whisper)\n",
    "- Text-to-speech (Tacotron, WaveNet)\n",
    "- Speaker identification and music tagging\n",
    "\n",
    "### ðŸ“ˆ Time Series & Finance\n",
    "- Stock prediction\n",
    "- Anomaly detection\n",
    "- Risk scoring and fraud detection\n",
    "\n",
    "### ðŸŽ® Reinforcement Learning\n",
    "- AlphaGo, OpenAI Five (games)\n",
    "- Robotics & autonomous navigation\n",
    "- Industrial control systems\n",
    "\n",
    "### ðŸŽ¨ Generative AI\n",
    "- Image generation: DALLÂ·E, Midjourney\n",
    "- Text generation: ChatGPT, Claude\n",
    "- Video generation: Sora, RunwayML\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Final Takeaways\n",
    "\n",
    "- Deep Learning is the engine behind modern AI\n",
    "- Neural Networks are the building blocks â€” evolving from feedforward to transformers\n",
    "- DL systems learn **end-to-end representations** from massive data\n",
    "- Key drivers: GPU compute, open datasets, and open-source libraries\n",
    "\n",
    "> \"DL is not magic â€” it's data, compute, and math â€” at scale.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9be1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
